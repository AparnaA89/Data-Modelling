{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, read_excel, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Ascending_Triangle\\\\Ascending_Triangle.csv')\n",
    "\n",
    "df['colon_count'] = df.pattern.str.count(':')\n",
    "df['end_count'] = df.pattern.str.count('end')\n",
    "\n",
    "new=df['pattern'].str.split(\":\",n=5, expand=True)\n",
    "df[\"none\"]= new[0]\n",
    "df[\"end\"]= new[1]\n",
    "df[\"lower\"]= new[2]\n",
    "df[\"start\"]= new[3]\n",
    "df[\"upper\"]= new[4]\n",
    "df[\"vertical\"]= new[5]\n",
    "\n",
    "df = df.drop(columns = \"none\")\n",
    "df['end']=df['end'].replace(\", 'lower_line'\",'',regex=True)\n",
    "df['lower']=df['lower'].replace(\", 'start_point'\",'',regex=True)\n",
    "df['start']=df['start'].replace(\", 'upper_line'\",'',regex=True)\n",
    "df['upper']=df['upper'].replace(\", 'vertical_line'\",'',regex=True)\n",
    "df['vertical']=df['vertical'].replace(v\"}]\",'',regex=True)\n",
    "\n",
    "new1=df['lower'].str.split(\"], \",n=1, expand=True)\n",
    "df[\"low_x\"]= new1[0]\n",
    "df[\"low_y\"]= new1[1]\n",
    "\n",
    "new2=df['upper'].str.split(\"], \",n=1, expand=True)\n",
    "df[\"upper_x\"]= new2[0]\n",
    "df[\"upper_y\"]= new2[1]\n",
    "\n",
    "new3=df['vertical'].str.split(\"], \",n=1, expand=True)\n",
    "df[\"vertical_x\"]= new3[0]\n",
    "df[\"vertical_y\"]= new3[1]\n",
    "\n",
    "df['low_x']=df['low_x'].str.replace(\"[\",'')\n",
    "df['low_y']=df['low_y'].str.replace(\"]\",'')\n",
    "df['low_y']=df['low_y'].str.replace(\"[\",'')\n",
    "df['upper_x']=df['upper_x'].str.replace(\"[\",'')\n",
    "df['upper_y']=df['upper_y'].str.replace(\"]\",'')\n",
    "df['upper_y']=df['upper_y'].str.replace(\"[\",'')\n",
    "df['vertical_x']=df['vertical_x'].str.replace(\"[\",'')\n",
    "df['vertical_y']=df['vertical_y'].str.replace(\"]\",'')\n",
    "df['vertical_y']=df['vertical_y'].str.replace(\"[\",'')\n",
    "\n",
    "df = df.drop(['lower','upper','vertical'], axis = 1)\n",
    "df.to_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Ascending_Triangle\\\\Ascending_Triangle_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Bearish_Channel\\\\Bearish_channel.csv')\n",
    "\n",
    "df1['colon_count'] = df1.pattern.str.count(':')\n",
    "df1['end_count'] = df1.pattern.str.count('end')\n",
    "\n",
    "new=df1['pattern'].str.split(\":\",n=4, expand=True)\n",
    "df1['none']= new[0]\n",
    "df1['end']= new[1]\n",
    "df1['lower']= new[2]\n",
    "df1['start']= new[3]\n",
    "df1['upper']= new[4]\n",
    "\n",
    "df1 = df1.drop(columns = \"none\")\n",
    "df1['end']=df1['end'].replace(\", 'lower_line'\",'',regex=True)\n",
    "df1['lower']=df1['lower'].replace(\", 'start_point'\",'',regex=True)\n",
    "df1['start']=df1['start'].replace(\", 'upper_line'\",'',regex=True)\n",
    "df1['upper']=df1['upper'].replace(\"]]}]\",'',regex=True)\n",
    "\n",
    "new1=df1['lower'].str.split(\"], \",n=1, expand=True)\n",
    "df1[\"low_x\"]= new1[0]\n",
    "df1[\"low_y\"]= new1[1]\n",
    "\n",
    "new2=df1['upper'].str.split(\"], \",n=1, expand=True)\n",
    "df1[\"upper_x\"]= new2[0]\n",
    "df1[\"upper_y\"]= new2[1]\n",
    "\n",
    "df1['low_x']=df1['low_x'].str.replace(\"[\",'')\n",
    "df1['low_y']=df1['low_y'].str.replace(\"]\",'')\n",
    "df1['low_y']=df1['low_y'].str.replace(\"[\",'')\n",
    "df1['upper_x']=df1['upper_x'].str.replace(\"[\",'')\n",
    "df1['upper_y']=df1['upper_y'].str.replace(\"]\",'')\n",
    "df1['upper_y']=df1['upper_y'].str.replace(\"[\",'')\n",
    "\n",
    "df1.to_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Bearish_Channel\\\\Bearish_channel_updated.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Bullish_Channel\\\\Bullish_Channel.csv')\n",
    "df2['colon_count'] = df2.pattern.str.count(':')\n",
    "df2['end_count'] = df2.pattern.str.count('end')\n",
    "new=df2['pattern'].str.split(\":\",n=4, expand=True)\n",
    "df2['none']= new[0]\n",
    "df2['end']= new[1]\n",
    "df2['lower']= new[2]\n",
    "df2['start']= new[3]\n",
    "df2['upper']= new[4]\n",
    "df2 = df2.drop(columns = \"none\")\n",
    "df2['end']=df2['end'].replace(\", 'lower_line'\",'',regex=True)\n",
    "df2['lower']=df2['lower'].replace(\", 'start_point'\",'',regex=True)\n",
    "df2['start']=df2['start'].replace(\", 'upper_line'\",'',regex=True)\n",
    "df2['upper']=df2['upper'].replace(\"]]}]\",'',regex=True)\n",
    "\n",
    "new1=df2['lower'].str.split(\"], \",n=1, expand=True)\n",
    "df2[\"low_x\"]= new1[0]\n",
    "df2[\"low_y\"]= new1[1]\n",
    "\n",
    "new2=df2['upper'].str.split(\"], \",n=1, expand=True)\n",
    "df2[\"upper_x\"]= new2[0]\n",
    "df2[\"upper_y\"]= new2[1]\n",
    "\n",
    "df2['low_x']=df2['low_x'].str.replace(\"[\",'')\n",
    "df2['low_y']=df2['low_y'].str.replace(\"]\",'')\n",
    "df2['low_y']=df2['low_y'].str.replace(\"[\",'')\n",
    "df2['upper_x']=df2['upper_x'].str.replace(\"[\",'')\n",
    "df2['upper_y']=df2['upper_y'].str.replace(\"]\",'')\n",
    "df2['upper_y']=df2['upper_y'].str.replace(\"[\",'')\n",
    "\n",
    "df2.to_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Bullish_Channel\\\\Bullish_Channel_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Cup_With_Handle\\\\Cup_With_Handle.csv')\n",
    "df3['colon_count'] = df3.pattern.str.count(':')\n",
    "df3['end_count'] = df3.pattern.str.count('end')\n",
    "new=df3['pattern'].str.split(\":\",n=5, expand=True)\n",
    "df3['none']= new[0]\n",
    "df3['cup_bottom_point']= new[1]\n",
    "df3['cup_start_point']= new[2]\n",
    "df3['end_point']= new[3]\n",
    "df3['handle_start_point']= new[4]\n",
    "df3['start_point']= new[5]\n",
    "\n",
    "df3 = df3.drop(columns = \"none\")\n",
    "df3['cup_bottom_point']=df3['cup_bottom_point'].replace(\", 'cup_start_point'\",'',regex=True)\n",
    "df3['cup_start_point']=df3['cup_start_point'].replace(\", 'end_point'\",'',regex=True)\n",
    "df3['end_point']=df3['end_point'].replace(\", 'handle_start_point'\",'',regex=True)\n",
    "df3['handle_start_point']=df3['handle_start_point'].replace(\", 'start_point'\",'',regex=True)\n",
    "df3['start_point']=df3['start_point'].replace(\"}]\",'',regex=True)\n",
    "\n",
    "df3.to_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Cup_With_Handle\\\\Cup_With_Handle_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Descending_Triangle\\\\Descending_Triangle.csv')\n",
    "\n",
    "df4['colon_count'] = df4.pattern.str.count(':')\n",
    "df4['end_count'] = df4.pattern.str.count('end')\n",
    "\n",
    "new=df4['pattern'].str.split(\":\",n=5, expand=True)\n",
    "df4[\"none\"]= new[0]\n",
    "df4[\"end\"]= new[1]\n",
    "df4[\"lower\"]= new[2]\n",
    "df4[\"start\"]= new[3]\n",
    "df4[\"upper\"]= new[4]\n",
    "df4[\"vertical\"]= new[5]\n",
    "\n",
    "df4 = df4.drop(columns = \"none\")\n",
    "df4['end']=df4['end'].replace(\", 'lower_line'\",'',regex=True)\n",
    "df4['lower']=df4['lower'].replace(\", 'start_point'\",'',regex=True)\n",
    "df4['start']=df4['start'].replace(\", 'upper_line'\",'',regex=True)\n",
    "df4['upper']=df4['upper'].replace(\", 'vertical_line'\",'',regex=True)\n",
    "df4['vertical']=df4['vertical'].replace(\"}]\",'',regex=True)\n",
    "\n",
    "new1=df4['lower'].str.split(\"], \",n=1, expand=True)\n",
    "df4[\"low_x\"]= new1[0]\n",
    "df4[\"low_y\"]= new1[1]\n",
    "\n",
    "new2=df4['upper'].str.split(\"], \",n=1, expand=True)\n",
    "df4[\"upper_x\"]= new2[0]\n",
    "df4[\"upper_y\"]= new2[1]\n",
    "\n",
    "new3=df4['vertical'].str.split(\"], \",n=1, expand=True)\n",
    "df4[\"vertical_x\"]= new3[0]\n",
    "df4[\"vertical_y\"]= new3[1]\n",
    "\n",
    "df4['low_x']=df4['low_x'].str.replace(\"[\",'')\n",
    "df4['low_y']=df4['low_y'].str.replace(\"]\",'')\n",
    "df4['low_y']=df4['low_y'].str.replace(\"[\",'')\n",
    "df4['low_y']=df4['low_y'].str.replace(\", 'start'\",'')\n",
    "df4['upper_x']=df4['upper_x'].str.replace(\"[\",'')\n",
    "df4['upper_y']=df4['upper_y'].str.replace(\"]\",'')\n",
    "df4['upper_y']=df4['upper_y'].str.replace(\"[\",'')\n",
    "df4['vertical_x']=df4['vertical_x'].str.replace(\"[\",'')\n",
    "df4['vertical_y']=df4['vertical_y'].str.replace(\"]\",'')\n",
    "df4['vertical_y']=df4['vertical_y'].str.replace(\"[\",'')\n",
    "\n",
    "df4 = df4.drop(['lower','upper','vertical'], axis = 1)\n",
    "df4.to_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Descending_Triangle\\\\Descending_Triangle_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('D:\\Learning\\Time Series Analysis - Real time data\\Double_Bottom\\Double_Bottom.csv')\n",
    "\n",
    "df5['colon_count'] = df5.pattern.str.count(':')\n",
    "df5['end_count'] = df5.pattern.str.count('end')\n",
    "\n",
    "new=df5['pattern'].str.split(\":\",n=24, expand=True)\n",
    "df5[\"none\"]= new[0]\n",
    "df5[\"end\"]= new[1]\n",
    "df5[\"left_bottom\"]= new[2]\n",
    "df5[\"mid_point\"]= new[3]\n",
    "df5[\"right_bottom\"]= new[4]\n",
    "df5[\"start_point\"]= new[5]\n",
    "df5[\"target_price\"]= new[6]\n",
    "df5[\"end1\"]= new[7]\n",
    "df5[\"left_bottom1\"]= new[8]\n",
    "df5[\"mid_point1\"]= new[9]\n",
    "df5[\"right_bottom1\"]= new[10]\n",
    "df5[\"start_point1\"]= new[11]\n",
    "df5[\"target_price1\"]= new[12]\n",
    "df5[\"end2\"]= new[13]\n",
    "df5[\"left_bottom2\"]= new[14]\n",
    "df5[\"mid_point2\"]= new[15]\n",
    "df5[\"right_bottom2\"]= new[16]\n",
    "df5[\"start_point2\"]= new[17]\n",
    "df5[\"target_price2\"]= new[18]\n",
    "df5[\"end3\"]= new[19]\n",
    "df5[\"left_bottom3\"]= new[20]\n",
    "df5[\"mid_point3\"]= new[21]\n",
    "df5[\"right_bottom3\"]= new[22]\n",
    "df5[\"start_point3\"]= new[23]\n",
    "df5[\"target_price3\"]= new[24]\n",
    "\n",
    "df5 = df5.drop(columns = \"none\")\n",
    "df5['end']=df5['end'].replace(\", 'left_bottom'\",'',regex=True)\n",
    "df5[\"left_bottom\"]= df5['left_bottom'].replace(\", 'mid_point'\",'',regex=True)\n",
    "df5[\"mid_point\"]= df5[\"mid_point\"].replace(\", 'right_bottom'\",'',regex=True)\n",
    "df5[\"right_bottom\"]= df5[\"right_bottom\"].replace(\", 'start_point'\",'',regex=True)\n",
    "df5[\"start_point\"]= df5[\"start_point\"].replace(\", 'target_price'\",'',regex=True)\n",
    "df5[\"target_price\"]= df5[\"target_price\"].replace(\"}, {'end_point'\",'',regex=True)\n",
    "df5[\"target_price\"]= df5[\"target_price\"].replace(\"}]'end_point'\",'',regex=True)\n",
    "df5[\"end1\"]= df5['end1'].replace(\", 'left_bottom'\",'',regex=True)\n",
    "df5[\"left_bottom1\"]= df5['left_bottom1'].replace(\", 'mid_point'\",'',regex=True)\n",
    "df5[\"mid_point1\"]= df5[\"mid_point1\"].replace(\", 'right_bottom'\",'',regex=True)\n",
    "df5[\"right_bottom1\"]= df5[\"right_bottom1\"].replace(\", 'start_point'\",'',regex=True)\n",
    "df5[\"start_point1\"]= df5[\"start_point1\"].replace(\", 'target_price'\",'',regex=True)\n",
    "df5[\"target_price1\"]= df5[\"target_price1\"].replace(\"}, {'end_point'\",'',regex=True)\n",
    "df5[\"target_price1\"]= df5[\"target_price1\"].replace(\"}]'end_point'\",'',regex=True)\n",
    "df5[\"target_price1\"]= df5[\"target_price1\"].replace(\"}]\",'',regex=True)\n",
    "df5[\"end2\"]= df5['end2'].replace(\", 'left_bottom'\",'',regex=True)\n",
    "df5[\"left_bottom2\"]= df5['left_bottom2'].replace(\", 'mid_point'\",'',regex=True)\n",
    "df5[\"mid_point2\"]= df5[\"mid_point2\"].replace(\", 'right_bottom'\",'',regex=True)\n",
    "df5[\"right_bottom2\"]= df5[\"right_bottom2\"].replace(\", 'start_point'\",'',regex=True)\n",
    "df5[\"start_point2\"]= df5[\"start_point2\"].replace(\", 'target_price'\",'',regex=True)\n",
    "df5[\"target_price2\"]= df5[\"target_price2\"].replace(\"}, {'end_point'\",'',regex=True)\n",
    "df5[\"target_price2\"]= df5[\"target_price2\"].replace(\"}]'end_point'\",'',regex=True)\n",
    "df5[\"target_price2\"]= df5[\"target_price1\"].replace(\"}]\",'',regex=True)\n",
    "df5[\"end3\"]= df5['end3'].replace(\", 'left_bottom'\",'',regex=True)\n",
    "df5[\"left_bottom3\"]= df5['left_bottom3'].replace(\", 'mid_point'\",'',regex=True)\n",
    "df5[\"mid_point3\"]= df5[\"mid_point3\"].replace(\", 'right_bottom'\",'',regex=True)\n",
    "df5[\"right_bottom3\"]= df5[\"right_bottom3\"].replace(\", 'start_point'\",'',regex=True)\n",
    "df5[\"start_point3\"]= df5[\"start_point3\"].replace(\", 'target_price'\",'',regex=True)\n",
    "df5[\"target_price3\"]= df5[\"target_price3\"].replace(\"}]\",'',regex=True)\n",
    "\n",
    "df5.to_csv('D:\\Learning\\Time Series Analysis - Real time data\\Double_Bottom\\Double_Bottom_updated.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Double_Top\\\\Double_Top.csv')\n",
    "\n",
    "df6['colon_count'] = df6.pattern.str.count(':')\n",
    "df6['end_count'] = df6.pattern.str.count('end')\n",
    "\n",
    "new=df6['pattern'].str.split(\":\",n=12, expand=True)\n",
    "df6[\"none\"]= new[0]\n",
    "df6[\"end\"]= new[1]\n",
    "df6[\"left_top\"]= new[2]\n",
    "df6[\"mid_point\"]= new[3]\n",
    "df6[\"right_top\"]= new[4]\n",
    "df6[\"start_point\"]= new[5]\n",
    "df6[\"target_price\"]= new[6]\n",
    "df6[\"end1\"]= new[7]\n",
    "df6[\"left_top1\"]= new[8]\n",
    "df6[\"mid_point1\"]= new[9]\n",
    "df6[\"right_top1\"]= new[10]\n",
    "df6[\"start_point1\"]= new[11]\n",
    "df6[\"target_price1\"]= new[12]\n",
    "\n",
    "df6 = df6.drop(columns = \"none\")\n",
    "df6['end']=df6['end'].replace(\", 'left_top'\",'',regex=True)\n",
    "df6[\"left_top\"]= df6['left_top'].replace(\", 'mid_point'\",'',regex=True)\n",
    "df6[\"mid_point\"]= df6[\"mid_point\"].replace(\", 'right_top'\",'',regex=True)\n",
    "df6[\"right_top\"]= df6[\"right_top\"].replace(\", 'start_point'\",'',regex=True)\n",
    "df6[\"start_point\"]= df6[\"start_point\"].replace(\", 'target_price'\",'',regex=True)\n",
    "df6[\"target_price\"]= df6[\"target_price\"].replace(\", {'end_point'\",'',regex=True)\n",
    "df6[\"target_price\"]= df6[\"target_price\"].replace(\"}]\",'',regex=True)\n",
    "df6[\"end1\"]= df6['end1'].replace(\", 'left_top'\",'',regex=True)\n",
    "df6[\"left_top1\"]= df6['left_top1'].replace(\", 'mid_point'\",'',regex=True)\n",
    "df6[\"mid_point1\"]= df6[\"mid_point1\"].replace(\", 'right_top'\",'',regex=True)\n",
    "df6[\"right_top1\"]= df6[\"right_top1\"].replace(\", 'start_point'\",'',regex=True)\n",
    "df6[\"start_point1\"]= df6[\"start_point1\"].replace(\", 'target_price'\",'',regex=True)\n",
    "df6[\"target_price1\"]= df6[\"target_price1\"].replace(\"}]\",'',regex=True)\n",
    "\n",
    "df6.to_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Double_Top\\\\Double_Top_updated.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.read_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Flat_Base\\\\Flat_Base.csv')\n",
    "\n",
    "df7['colon_count'] = df7.pattern.str.count(':')\n",
    "df7['end_count'] = df7.pattern.str.count('end')\n",
    "\n",
    "new=df7['pattern'].str.split(\":\",n=5, expand=True)\n",
    "df7[\"none\"]= new[0]\n",
    "df7['end']=new[1]\n",
    "df7[\"line_start\"]= new[2]\n",
    "df7[\"lower_line_y_val\"]= new[3]\n",
    "df7[\"start_point\"]= new[4]\n",
    "df7[\"upper_line_y_val\"]= new[5]\n",
    "\n",
    "df7 = df7.drop(columns = \"none\")\n",
    "df7['end']=df7['end'].replace(\", 'line_start'\",'',regex=True)\n",
    "df7[\"line_start\"]= df7['line_start'].replace(\", 'lower_line_y_val'\",'',regex=True)\n",
    "df7[\"lower_line_y_val\"]= df7[\"lower_line_y_val\"].replace(\", 'start_point'\",'',regex=True)\n",
    "df7[\"start_point\"]= df7[\"start_point\"].replace(\", 'upper_line_y_val'\",'',regex=True)\n",
    "df7[\"upper_line_y_val\"]= df7[\"upper_line_y_val\"].replace(\"}]\",'',regex=True)\n",
    "\n",
    "df7.to_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Flat_Base\\\\Flat_Base_updated.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = pd.read_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Head_and_Shoulder\\\\Head_and_Shoulder.csv')\n",
    "\n",
    "df8['colon_count'] = df8.pattern.str.count(':')\n",
    "df8['end_count'] = df8.pattern.str.count('end')\n",
    "\n",
    "new=df8['pattern'].str.split(\":\",n=7, expand=True)\n",
    "df8[\"none\"]= new[0]\n",
    "df8['end']=new[1]\n",
    "df8[\"head\"]= new[2]\n",
    "df8[\"left_shoulder\"]= new[3]\n",
    "df8[\"line\"]= new[4]\n",
    "df8[\"right_shoulder\"]= new[5]\n",
    "df8[\"start\"]= new[6]\n",
    "df8[\"target\"]= new[7]\n",
    "\n",
    "df8 = df8.drop(columns = \"none\")\n",
    "df8['end']=df8['end'].replace(\", 'head'\",'',regex=True)\n",
    "df8[\"head\"]= df8['head'].replace(\", 'left_shoulder'\",'',regex=True)\n",
    "df8['head']=df8['head'].str.replace(\"[\",'')\n",
    "df8['head']=df8['head'].str.replace(\"]\",'')\n",
    "df8[\"left_shoulder\"]= df8[\"left_shoulder\"].replace(\", 'line'\",'',regex=True)\n",
    "df8[\"left_shoulder\"]= df8['left_shoulder'].str.replace(\"[\",'')\n",
    "df8['left_shoulder']=df8['left_shoulder'].str.replace(\"]\",'')\n",
    "df8[\"line\"]= df8[\"line\"].replace(\", 'right_shoulder'\",'',regex=True)\n",
    "df8[\"right_shoulder\"]= df8[\"right_shoulder\"].replace(\", 'start_point'\",'',regex=True)\n",
    "df8[\"right_shoulder\"]= df8['right_shoulder'].str.replace(\"[\",'')\n",
    "df8['right_shoulder']=df8['right_shoulder'].str.replace(\"]\",'')\n",
    "df8[\"start\"]= df8[\"start\"].replace(\", 'target'\",'',regex=True)\n",
    "df8[\"target\"]= df8[\"target\"].replace(\"}]\",'',regex=True)\n",
    "\n",
    "\n",
    "new1=df8[\"line\"].str.split(\"], \",n=1, expand=True)\n",
    "df8[\"line_x\"]= new1[0]\n",
    "df8[\"line_y\"]= new1[1]\n",
    "\n",
    "df8['line_x']=df8['line_x'].str.replace(\"[\",'',regex=True)\n",
    "df8['line_y']=df8['line_y'].str.replace(\"]\",'',regex=True)\n",
    "df8['line_y']=df8['line_y'].str.replace(\"[\",'',regex=True)\n",
    "\n",
    "df8.to_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Head_and_Shoulder\\\\Head_and_Shoulder_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = pd.read_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Triple_Bottom\\\\Triple_Bottom.csv')\n",
    "\n",
    "df9['colon_count'] = df9.pattern.str.count(':')\n",
    "df9['end_count'] = df9.pattern.str.count('end')\n",
    "\n",
    "new=df9['pattern'].str.split(\":\",n=6, expand=True)\n",
    "df9[\"none\"]= new[0]\n",
    "df9['end']=new[1]\n",
    "df9[\"left\"]= new[2]\n",
    "df9[\"middle_part\"]= new[3]\n",
    "df9[\"right_part\"]= new[4]\n",
    "df9[\"start\"]= new[5]\n",
    "df9[\"target\"]= new[6]\n",
    "\n",
    "df9 = df9.drop(columns = \"none\")\n",
    "df9['end']=df9['end'].replace(\", 'left_part'\",'',regex=True)\n",
    "df9[\"left\"]= df9['left'].replace(\", 'middle_part'\",'',regex=True)\n",
    "df9[\"left\"]= df9['left'].str.replace(\"[\",'')\n",
    "df9[\"left\"]= df9['left'].str.replace(\"]\",'')\n",
    "df9[\"middle_part\"]= df9[\"middle_part\"].replace(\", 'right_part'\",'',regex=True)\n",
    "df9[\"middle_part\"]= df9['middle_part'].str.replace(\"[\",'')\n",
    "df9[\"middle_part\"]= df9['middle_part'].str.replace(\"]\",'')\n",
    "df9[\"right_part\"]= df9[\"right_part\"].replace(\", 'start_point'\",'',regex=True)\n",
    "df9[\"right_part\"]= df9['right_part'].str.replace(\"[\",'')\n",
    "df9[\"right_part\"]= df9['right_part'].str.replace(\"]\",'')\n",
    "df9[\"start\"]= df9[\"start\"].replace(\", 'target_point'\",'',regex=True)\n",
    "df9[\"target\"]= df9[\"target\"].replace(\"}]\",'',regex=True)\n",
    "\n",
    "df9.to_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Triple_Bottom\\\\Triple_Bottom_updated.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.read_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Triple_Top\\\\Triple_Top.csv')\n",
    "\n",
    "df10['colon_count'] = df10.pattern.str.count(':')\n",
    "df10['end_count'] = df10.pattern.str.count('end')\n",
    "\n",
    "new=df10['pattern'].str.split(\":\",n=6, expand=True)\n",
    "df10[\"none\"]= new[0]\n",
    "df10['end']=new[1]\n",
    "df10[\"left\"]= new[2]\n",
    "df10[\"middle_part\"]= new[3]\n",
    "df10[\"right_part\"]= new[4]\n",
    "df10[\"start\"]= new[5]\n",
    "df10[\"target\"]= new[6]\n",
    "\n",
    "df10 = df10.drop(columns = \"none\")\n",
    "df10['end']=df10['end'].replace(\", 'left_part'\",'',regex=True)\n",
    "df10[\"left\"]= df10['left'].replace(\", 'middle_part'\",'',regex=True)\n",
    "df10[\"left\"]= df10['left'].str.replace(\"[\",'')\n",
    "df10[\"left\"]= df10['left'].str.replace(\"]\",'')\n",
    "df10[\"middle_part\"]= df10[\"middle_part\"].replace(\", 'right_part'\",'',regex=True)\n",
    "df10[\"middle_part\"]= df10['middle_part'].str.replace(\"[\",'')\n",
    "df10[\"middle_part\"]= df10['middle_part'].str.replace(\"]\",'')\n",
    "df10[\"right_part\"]= df10[\"right_part\"].replace(\", 'start_point'\",'',regex=True)\n",
    "df10[\"right_part\"]= df10['right_part'].str.replace(\"[\",'')\n",
    "df10[\"right_part\"]= df10['right_part'].str.replace(\"]\",'')\n",
    "df10[\"start\"]= df10[\"start\"].replace(\", 'target_point'\",'',regex=True)\n",
    "df10[\"target\"]= df10[\"target\"].replace(\"}]\",'',regex=True)\n",
    "\n",
    "df10.to_csv('D:\\\\Learning\\\\Time Series Analysis - Real time data\\\\Triple_Top\\\\Triple_Top_updated.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
